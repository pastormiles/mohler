#!/usr/bin/env python3
"""
Step 3: Extract YouTube Transcripts with Rotating Proxies
Version: 10.0
Updated: Uses centralized config.py for all project-specific settings.

================================================================================
OVERVIEW
================================================================================
This script extracts transcripts (captions/subtitles) from YouTube videos using
the youtube-transcript-api library combined with Webshare rotating residential
proxies to avoid YouTube's IP-based rate limiting.

Why proxies are needed:
- YouTube aggressively rate-limits transcript requests from single IPs
- After ~50-100 requests, YouTube blocks the IP for hours or days
- Webshare provides a pool of residential IPs that rotate automatically
- This allows sustained extraction of thousands of transcripts

================================================================================
HOW IT WORKS
================================================================================
1. Loads video list from metadata file (output of step 02)
2. Tracks progress in progress.json to enable resume after interruption
3. For each video:
   a. Creates API instance with Webshare proxy
   b. Attempts to fetch English transcript (manual > auto-generated > translated)
   c. Saves transcript segments to {video_id}.json
   d. Updates progress tracking
4. Handles rate limiting with retries and exponential backoff
5. Stops gracefully after 10 consecutive blocks

================================================================================
TRANSCRIPT PRIORITY
================================================================================
The script attempts to get transcripts in this order:
1. Manually created English transcript (highest quality)
2. Auto-generated English transcript (YouTube's speech-to-text)
3. Any available transcript translated to English
4. Any available transcript in original language

================================================================================
OUTPUT FORMAT
================================================================================
Each transcript is saved as {video_id}.json with structure:
{
    "video_id": "abc123",
    "title": "Video Title",
    "channel": "Channel Name",
    "duration_seconds": 1234,
    "language": "en",
    "is_generated": true,
    "segment_count": 150,
    "segments": [
        {"text": "Hello world", "start": 0.0, "duration": 2.5},
        {"text": "Next line", "start": 2.5, "duration": 3.0},
        ...
    ],
    "extracted_at": "2025-01-01T12:00:00",
    "extraction_method": "youtube-transcript-api-webshare"
}

================================================================================
USAGE
================================================================================
    python 03_extract_transcripts_v10.py                  # Process all videos
    python 03_extract_transcripts_v10.py --limit 10       # Process first 10 unprocessed
    python 03_extract_transcripts_v10.py --retry-blocked  # Retry previously blocked videos
    python 03_extract_transcripts_v10.py --delay 3.0      # Custom delay between requests
    python 03_extract_transcripts_v10.py --test           # Test with a known video
    python 03_extract_transcripts_v10.py --test VIDEO_ID  # Test with specific video

================================================================================
REQUIREMENTS
================================================================================
    - youtube-transcript-api (pip install youtube-transcript-api)
    - Output from 02_fetch_video_metadata.py (video_metadata.json)
    - Webshare account with rotating residential proxies
    - WEBSHARE_PROXY_USERNAME and WEBSHARE_PROXY_PASSWORD in .env file
    - config.py in project root (generated by yt_ai_search_setup.sh)

================================================================================
SETUP
================================================================================
This script expects to be in the /scripts directory of a project created
by yt_ai_search_setup.sh. The project structure should be:

{PROJECT}/
├── config.py                  <- Project settings (auto-generated)
├── .env                       <- API keys including Webshare credentials
├── scripts/
│   └── 03_extract_transcripts_v10.py  <- This script
└── data/
    ├── metadata/
    │   └── video_metadata.json        <- Input (from step 02)
    └── transcripts/
        ├── progress.json              <- Progress tracking
        └── {video_id}.json            <- Output (one per video)

================================================================================
WEBSHARE SETUP
================================================================================
1. Create account at webshare.io
2. Purchase rotating residential proxies (cheapest tier is ~$6/month)
3. Go to Dashboard > Proxy > Settings
4. Copy your proxy username and password
5. Add to your .env file:
   WEBSHARE_PROXY_USERNAME=your_username
   WEBSHARE_PROXY_PASSWORD=your_password

The youtube-transcript-api has built-in WebshareProxyConfig that handles
the proxy rotation automatically - no additional setup required.

================================================================================
TROUBLESHOOTING
================================================================================
- "10 consecutive blocks": YouTube is blocking aggressively. Wait 1-2 hours
  and run again with --retry-blocked flag.
  
- "No transcript available": Video has no captions. This is common for older
  videos or videos in non-English languages without auto-captions.
  
- Slow extraction: Increase --delay to be gentler on the API, or decrease
  it if you have a premium Webshare plan with more bandwidth.

================================================================================
"""

import sys
import json
import random
import time
import logging
import argparse
from pathlib import Path
from datetime import datetime

# Add project root to path for config import
SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent
sys.path.insert(0, str(PROJECT_ROOT))

# =============================================================================
# CONFIG IMPORTS
# =============================================================================
# All project-specific settings come from config.py, which is generated by
# yt_ai_search_setup.sh and reads API keys from .env
from config import (
    # Channel settings (for logging)
    CHANNEL_HANDLE,
    CHANNEL_DISPLAY_NAME,
    # Proxy credentials (from .env)
    WEBSHARE_PROXY_USERNAME,
    WEBSHARE_PROXY_PASSWORD,
    # Paths
    METADATA_FILE,           # Input: video_metadata.json
    TRANSCRIPTS_DIR,         # Output directory for {video_id}.json files
    TRANSCRIPT_PROGRESS_FILE, # Progress tracking: progress.json
    LOGS_DIR,
    # Processing settings
    TRANSCRIPT_DELAY,        # Default delay between requests
    TRANSCRIPT_MAX_RETRIES,  # Max retries for rate-limited requests
    # Helper functions
    ensure_directories,
    get_log_file,
)

# =============================================================================
# YOUTUBE TRANSCRIPT API IMPORTS
# =============================================================================
from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api.proxies import WebshareProxyConfig
from youtube_transcript_api._errors import (
    TranscriptsDisabled,     # Video has captions disabled by uploader
    NoTranscriptFound,       # No captions available in any language
    VideoUnavailable         # Video is private, deleted, or region-locked
)


def setup_logging():
    """
    Configure logging with both file and console output.
    
    Log file is stored in the project's logs directory with a consistent
    naming pattern for easy identification.
    """
    ensure_directories()
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(get_log_file('03_extract_transcripts')),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)


logger = setup_logging()


# =============================================================================
# WEBSHARE PROXY CONFIGURATION
# =============================================================================

def get_api_instance():
    """
    Create a YouTubeTranscriptApi instance configured with Webshare proxies.
    
    The WebshareProxyConfig automatically handles:
    - Proxy rotation (new IP for each request)
    - Authentication with Webshare servers
    - Failover to backup proxies if primary fails
    
    Returns:
        YouTubeTranscriptApi: Configured API instance ready for requests
        
    Raises:
        ValueError: If proxy credentials are not configured in .env
    """
    if not WEBSHARE_PROXY_USERNAME or not WEBSHARE_PROXY_PASSWORD:
        raise ValueError(
            "Webshare proxy credentials not found!\n"
            "Add WEBSHARE_PROXY_USERNAME and WEBSHARE_PROXY_PASSWORD to your .env file.\n"
            "Get credentials from: https://www.webshare.io/"
        )
    
    return YouTubeTranscriptApi(
        proxy_config=WebshareProxyConfig(
            proxy_username=WEBSHARE_PROXY_USERNAME,
            proxy_password=WEBSHARE_PROXY_PASSWORD,
        )
    )


# =============================================================================
# DATA LOADING AND PROGRESS TRACKING
# =============================================================================

def load_metadata():
    """
    Load video metadata from step 02 output.
    
    Returns:
        dict: Mapping of video_id -> video metadata dict
        
    Raises:
        FileNotFoundError: If metadata file doesn't exist
    """
    if not METADATA_FILE.exists():
        raise FileNotFoundError(
            f"Metadata file not found: {METADATA_FILE}\n"
            "Please run 02_fetch_video_metadata.py first."
        )
    
    with open(METADATA_FILE, 'r') as f:
        data = json.load(f)
    
    videos_list = data.get("videos", [])
    return {video["video_id"]: video for video in videos_list}


def load_progress():
    """
    Load progress tracking from JSON file.
    
    Progress tracks videos in four categories:
    - completed: Successfully extracted transcripts
    - failed: Encountered errors during extraction
    - no_transcript: Video has no captions available
    - blocked: Rate limited by YouTube, needs retry later
    
    Returns:
        dict: Progress tracking data with lists for each category
    """
    if TRANSCRIPT_PROGRESS_FILE.exists():
        with open(TRANSCRIPT_PROGRESS_FILE, 'r') as f:
            data = json.load(f)
            # Ensure all expected keys exist (for backward compatibility)
            if "blocked" not in data:
                data["blocked"] = []
            return data
    
    # Initialize empty progress if file doesn't exist
    return {"completed": [], "failed": [], "no_transcript": [], "blocked": []}


def save_progress(progress):
    """
    Save progress tracking to JSON file.
    
    Called periodically during extraction and on completion to ensure
    progress is not lost if the script is interrupted.
    
    Args:
        progress: Progress dict to save
    """
    with open(TRANSCRIPT_PROGRESS_FILE, 'w') as f:
        json.dump(progress, f, indent=2)


def transcript_file_exists(video_id):
    """
    Check if a transcript file already exists for this video.
    
    Used to detect transcripts that were extracted but not recorded
    in progress.json (e.g., if script crashed after saving transcript
    but before updating progress).
    
    Args:
        video_id: YouTube video ID to check
        
    Returns:
        bool: True if transcript file exists
    """
    return (TRANSCRIPTS_DIR / f"{video_id}.json").exists()


# =============================================================================
# TRANSCRIPT EXTRACTION
# =============================================================================

def fetch_transcript_with_proxy(video_id, max_retries=None):
    """
    Fetch transcript for a video using Webshare rotating proxies.
    
    This function handles the full extraction flow:
    1. Connect to YouTube via Webshare proxy
    2. List available transcripts for the video
    3. Select best available transcript (manual > generated > translated)
    4. Download and format transcript segments
    5. Retry with backoff if rate limited
    
    Args:
        video_id: YouTube video ID (e.g., "dQw4w9WgXcQ")
        max_retries: Maximum retry attempts for rate-limited requests
                    (defaults to TRANSCRIPT_MAX_RETRIES from config)
    
    Returns:
        tuple: (segments, language, is_generated, error_type)
        - segments: List of dicts with 'text', 'start', 'duration' keys
        - language: Language code (e.g., 'en')
        - is_generated: True if auto-generated, False if manually created
        - error_type: None on success, or 'no_transcript'/'blocked'/'error'
    """
    if max_retries is None:
        max_retries = TRANSCRIPT_MAX_RETRIES
    
    last_error = None
    
    for attempt in range(max_retries):
        try:
            # Get fresh API instance (new proxy IP)
            ytt_api = get_api_instance()
            
            # Get list of available transcripts
            transcript_list = ytt_api.list(video_id)
            
            # Try to get transcript in priority order
            transcript = None
            is_generated = True
            
            # Priority 1: Manually created English transcript (highest quality)
            try:
                transcript = transcript_list.find_manually_created_transcript(['en'])
                is_generated = False
            except:
                pass
            
            # Priority 2: Auto-generated English transcript
            if not transcript:
                try:
                    transcript = transcript_list.find_generated_transcript(['en'])
                    is_generated = True
                except:
                    pass
            
            # Priority 3: Any available transcript, translated to English if possible
            if not transcript:
                try:
                    for t in transcript_list:
                        if t.is_translatable:
                            transcript = t.translate('en')
                            is_generated = t.is_generated
                            break
                        else:
                            # Use original language if translation not available
                            transcript = t
                            is_generated = t.is_generated
                            break
                except:
                    pass
            
            if transcript:
                # Fetch the actual transcript data
                transcript_data = transcript.fetch()
                
                # Convert to standard format
                segments = []
                for entry in transcript_data:
                    segments.append({
                        'text': entry.text,
                        'start': entry.start,
                        'duration': entry.duration
                    })
                
                return segments, transcript.language_code, is_generated, None
            else:
                return None, None, None, "no_transcript"
                
        except TranscriptsDisabled:
            # Video owner has disabled captions
            return None, None, None, "no_transcript"
            
        except NoTranscriptFound:
            # No captions available in any language
            return None, None, None, "no_transcript"
            
        except VideoUnavailable:
            # Video is private, deleted, or region-locked
            return None, None, None, "no_transcript"
            
        except Exception as e:
            error_msg = str(e).lower()
            
            # Check if it's a rate limit / blocking error
            if '429' in str(e) or 'too many' in error_msg or 'rate' in error_msg or 'blocked' in error_msg:
                last_error = f"Rate limited (attempt {attempt + 1})"
                logger.warning(f"  Attempt {attempt + 1} rate limited, retrying...")
                time.sleep(2 ** attempt)  # Exponential backoff: 1s, 2s, 4s...
                continue
            
            # Other error - might be transient
            last_error = str(e)
            logger.warning(f"  Attempt {attempt + 1} failed: {e}")
            time.sleep(1)
            continue
    
    # All retries exhausted - likely blocked
    return None, None, None, "blocked"


# =============================================================================
# MAIN EXTRACTION LOOP
# =============================================================================

def extract_all_transcripts(batch_size=100, delay=None, limit=None, retry_blocked=False):
    """
    Extract transcripts for all videos in the metadata file.
    
    This is the main extraction loop that:
    1. Determines which videos need processing
    2. Iterates through videos with progress tracking
    3. Handles rate limiting and errors gracefully
    4. Saves progress periodically for resume capability
    
    Args:
        batch_size: How often to save progress (every N videos)
        delay: Seconds to wait between requests (default from config)
        limit: Maximum number of videos to process (None = all)
        retry_blocked: If True, retry previously blocked videos instead
                      of processing new ones
    """
    if delay is None:
        delay = TRANSCRIPT_DELAY
    
    logger.info("=" * 60)
    logger.info("Starting Transcript Extraction")
    logger.info(f"Channel: {CHANNEL_DISPLAY_NAME} ({CHANNEL_HANDLE})")
    logger.info("=" * 60)
    logger.info(f"Using Webshare rotating residential proxies")
    
    # Validate proxy credentials
    if not WEBSHARE_PROXY_USERNAME or not WEBSHARE_PROXY_PASSWORD:
        logger.error("Webshare proxy credentials not configured!")
        logger.error("Add WEBSHARE_PROXY_USERNAME and WEBSHARE_PROXY_PASSWORD to .env")
        return
    
    # Load data
    metadata = load_metadata()
    progress = load_progress()
    
    # Determine which videos to process
    all_video_ids = list(metadata.keys())
    
    if retry_blocked:
        # Retry mode: process previously blocked videos
        to_process = progress.get("blocked", [])
        progress["blocked"] = []  # Clear blocked list (will re-add if still blocked)
        logger.info(f"Retrying {len(to_process)} previously blocked videos")
    else:
        # Normal mode: process unprocessed videos
        skip_set = set(
            progress["completed"] + 
            progress["failed"] + 
            progress["no_transcript"]
        )
        
        to_process = []
        skipped_existing = 0
        
        for vid in all_video_ids:
            # Skip if already in progress tracking
            if vid in skip_set:
                continue
            
            # Check for transcript files that weren't recorded in progress
            if transcript_file_exists(vid):
                progress["completed"].append(vid)
                skipped_existing += 1
                continue
            
            # Skip if currently blocked (use --retry-blocked to process these)
            if vid in progress.get("blocked", []):
                continue
            
            to_process.append(vid)
        
        if skipped_existing > 0:
            logger.info(f"Found {skipped_existing} existing transcript files (added to completed)")
            save_progress(progress)
    
    # Apply limit if specified
    if limit:
        to_process = to_process[:limit]
    
    # Log status summary
    logger.info(f"Total videos in metadata: {len(all_video_ids)}")
    logger.info(f"Already completed: {len(progress['completed'])}")
    logger.info(f"No transcript available: {len(progress['no_transcript'])}")
    logger.info(f"Currently blocked: {len(progress.get('blocked', []))}")
    logger.info(f"To process this run: {len(to_process)}")
    logger.info(f"Delay between requests: {delay}s")
    
    if not to_process:
        logger.info("No videos to process!")
        return
    
    # Statistics tracking
    stats = {
        "manual_transcripts": 0,
        "generated_transcripts": 0,
        "no_transcript": 0,
        "blocked": 0,
        "errors": 0
    }
    
    start_time = datetime.now()
    consecutive_errors = 0  # Track consecutive blocks for early stopping
    
    # Main extraction loop
    for i, video_id in enumerate(to_process, 1):
        video_info = metadata.get(video_id, {})
        title = video_info.get("title", "Unknown")[:50]
        
        # Fetch transcript with proxy
        segments, language, is_generated, error_type = fetch_transcript_with_proxy(video_id)
        
        if error_type == "blocked":
            # Rate limited - track for later retry
            consecutive_errors += 1
            stats["blocked"] += 1
            progress["blocked"].append(video_id)
            logger.warning(f"[{i}/{len(to_process)}] ⚠ {video_id} - {title}... (BLOCKED)")
            
            # Stop if too many consecutive blocks (indicates widespread blocking)
            if consecutive_errors >= 10:
                logger.error("10 consecutive blocks - stopping to avoid IP ban")
                logger.error("Wait 1-2 hours and run again with --retry-blocked")
                save_progress(progress)
                return
                
        elif error_type == "no_transcript":
            # No captions available for this video
            consecutive_errors = 0
            stats["no_transcript"] += 1
            progress["no_transcript"].append(video_id)
            logger.info(f"[{i}/{len(to_process)}] ✗ {video_id} - {title}... (no transcript)")
            
        elif error_type == "error":
            # Other error (network, parsing, etc.)
            consecutive_errors += 1
            stats["errors"] += 1
            progress["failed"].append(video_id)
            logger.info(f"[{i}/{len(to_process)}] ✗ {video_id} - {title}... (error)")
            
        elif segments:
            # Success! Save the transcript
            consecutive_errors = 0
            
            # Build transcript data structure
            transcript_data = {
                "video_id": video_id,
                "title": video_info.get("title"),
                "channel": video_info.get("channel_title"),
                "duration_seconds": video_info.get("duration_seconds"),
                "language": language,
                "is_generated": is_generated,
                "segment_count": len(segments),
                "segments": segments,
                "extracted_at": datetime.now().isoformat(),
                "extraction_method": "youtube-transcript-api-webshare"
            }
            
            # Save to file
            output_file = TRANSCRIPTS_DIR / f"{video_id}.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(transcript_data, f, ensure_ascii=False, indent=2)
            
            progress["completed"].append(video_id)
            
            # Track statistics
            if is_generated:
                stats["generated_transcripts"] += 1
                transcript_type = "generated"
            else:
                stats["manual_transcripts"] += 1
                transcript_type = "manual"
            
            logger.info(f"[{i}/{len(to_process)}] ✓ {video_id} - {title}... ({transcript_type}, {len(segments)} segments)")
        
        # Save progress periodically
        if i % batch_size == 0:
            save_progress(progress)
            elapsed = (datetime.now() - start_time).total_seconds()
            rate = i / elapsed * 3600  # Videos per hour
            remaining = len(to_process) - i
            eta_hours = remaining / rate if rate > 0 else 0
            logger.info(f"--- Progress saved. Rate: {rate:.0f}/hr, ETA: {eta_hours:.1f}h ---")
        
        # Delay between requests (with jitter to appear more human)
        actual_delay = delay + random.uniform(-0.5, 0.5)
        time.sleep(max(0.5, actual_delay))
    
    # Final save
    save_progress(progress)
    
    # Print summary
    elapsed = (datetime.now() - start_time).total_seconds()
    logger.info("\n" + "=" * 60)
    logger.info("EXTRACTION COMPLETE")
    logger.info("=" * 60)
    logger.info(f"Time elapsed: {elapsed/60:.2f} minutes")
    logger.info(f"Manual transcripts: {stats['manual_transcripts']}")
    logger.info(f"Generated transcripts: {stats['generated_transcripts']}")
    logger.info(f"No transcript available: {stats['no_transcript']}")
    logger.info(f"Blocked: {stats['blocked']}")
    logger.info(f"Errors: {stats['errors']}")
    total_success = stats['manual_transcripts'] + stats['generated_transcripts']
    logger.info(f"Total successful: {total_success}")
    if to_process:
        logger.info(f"Success rate: {total_success / len(to_process) * 100:.1f}%")


def test_single_video(video_id="dQw4w9WgXcQ"):
    """
    Test transcript extraction on a single video.
    
    Useful for:
    - Verifying proxy configuration is working
    - Testing before running full extraction
    - Debugging issues with specific videos
    
    Default test video is Rick Astley - Never Gonna Give You Up,
    which is guaranteed to have transcripts available.
    
    Args:
        video_id: YouTube video ID to test (default: Rick Roll)
    """
    print(f"\nTesting Webshare proxy extraction for: {video_id}")
    print("-" * 50)
    
    # Verify proxy credentials
    if not WEBSHARE_PROXY_USERNAME or not WEBSHARE_PROXY_PASSWORD:
        print("✗ ERROR: Webshare proxy credentials not configured!")
        print("  Add WEBSHARE_PROXY_USERNAME and WEBSHARE_PROXY_PASSWORD to .env")
        return
    
    segments, language, is_generated, error_type = fetch_transcript_with_proxy(video_id)
    
    if error_type == "blocked":
        print("⚠ BLOCKED - Proxy rate limited")
        print("  Wait a few minutes and try again")
    elif error_type == "no_transcript":
        print("✗ No transcript available for this video")
    elif error_type == "error":
        print("✗ Error occurred during extraction")
    elif segments:
        print(f"✓ Success!")
        print(f"  Language: {language}")
        print(f"  Type: {'Generated' if is_generated else 'Manual'}")
        print(f"  Segments: {len(segments)}")
        print(f"\n  First 3 segments:")
        for seg in segments[:3]:
            print(f"    [{seg['start']:.2f}s] {seg['text'][:60]}...")


def parse_args():
    """
    Parse command line arguments.
    
    Returns:
        argparse.Namespace: Parsed arguments
    """
    parser = argparse.ArgumentParser(
        description='Extract YouTube transcripts using Webshare rotating proxies',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python 03_extract_transcripts_v10.py                  # Process all videos
  python 03_extract_transcripts_v10.py --limit 10       # Process first 10
  python 03_extract_transcripts_v10.py --retry-blocked  # Retry blocked videos
  python 03_extract_transcripts_v10.py --test           # Test with default video
  python 03_extract_transcripts_v10.py --test abc123    # Test with specific video
        """
    )
    
    parser.add_argument(
        '--limit',
        type=int,
        default=None,
        help='Maximum number of videos to process (default: all)'
    )
    
    parser.add_argument(
        '--delay',
        type=float,
        default=None,
        help=f'Delay between requests in seconds (default: {TRANSCRIPT_DELAY})'
    )
    
    parser.add_argument(
        '--retry-blocked',
        action='store_true',
        help='Retry previously blocked videos instead of processing new ones'
    )
    
    parser.add_argument(
        '--test',
        nargs='?',
        const='dQw4w9WgXcQ',  # Default: Rick Roll
        metavar='VIDEO_ID',
        help='Test extraction on a single video (default: dQw4w9WgXcQ)'
    )
    
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()
    
    if args.test:
        test_single_video(args.test)
    else:
        extract_all_transcripts(
            limit=args.limit,
            delay=args.delay,
            retry_blocked=args.retry_blocked
        )
